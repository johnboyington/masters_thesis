\cleardoublepage

\chapter{Review of Available Methods and Data}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Beam Port Characterization Experiments}

%(Placeholder text)
%When measuring charged particles, the energy is directly measurable, which is nice.
%You can't do that with neutrons because they are neutral particles, which is less nice.
%Instead, most modern methods use discrete responses of individual detectors or detector configurations.

%Will talk about Bonner Spheres and Foil Activation here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Spectral Unfolding Methods}


% ------------------------------------------------------------------------------
\subsection{Formulation}


In practice, active and passive detectors and measurement techniques can appear very different.
However, for the purpose of mathematical formulation, it is possible, and indeed useful, to abstract their similitudes.
The above mentioned detection methods are resemblant in the fact that they provide a set of unique, discrete responses in the presence of an unknown, continuous neutron flux.
These discrete responses can vary greatly between different neutron environments.
For example, the LiI detection crystal of the BSS responds strongly in the presence of thermal neutrons and shows indifference towards neutrons of the faster variety, whereas certian reactions utilized in activation foil analysis, such as ($n, \alpha$), will not occur below specific, fast, threshold neutron energies, thus the detector is considered to have a fast response.
These largely energy-dependent responses, often related to material properties and reaction cross sections, can be considered functions.
A detector's `Response Function', now formally stated, is the measureable effect exhibited by a detector in a particular geometric configuration as a function of energy to a particular neutron source.
It is true that the response function is more technically also a function of parameters like geometry, source angle and position, etc.; however, it proves efficacious here to hold those details constant and consider response unidimensional in energy.

This conceptualization of response allows for a stated relationship between flux, response function and response for detector configuration $k$, shown in the following Fredholm integral equation of the first kind,

\begin{equation}\label{eqn:cont-response}
N_k + \epsilon_k = \int_E R_k(E) \phi(E) dE .
\end{equation}

Here, $N_k$ represents the measured response of the detector, $\epsilon_k$ is the unknown error associated with $N_k$, $R_k(E)$ is the response function, and $\phi(E)$ is the flux.
By placing a detector, $k$, with associated response function, $R(E)$, in a neutron environment, $\phi(E)$, it should exhibit some response $N_k$ that is dispaired $\epsilon_k$ from the expected response.

As posed, this formulation bears a certain number of issues.
It is impossible to derive a continuous function from a finite set of equations.
With any discrete number of measurements, the solution remains non-unique; the equation thus satisified by an infinite number of solutions.
The fact that the $\epsilon_k$ term is unknown implies that just as there is a limit to the confidence one can have in a real measurement, there too, is a limit on the intelligibility of the final spectrum.

A number of approximations serve to remedy these dilemmata.
An energy bin structure can be introduced, with energy bins $\Delta E_i (i = 1, \ldots, n)$, which discretize the problem.
Then, for a problem with $m$ detector configurations, equation \ref{eqn:cont-response} is rewritten as

\begin{equation}\label{eqn:disc-response}
N_k + \epsilon_k = \sum_i R_{ki} \phi_i, \quad k = 1,\ldots, m .
\end{equation}

Response functions are generally computed, using either deterministic or Monte Carlo methods, and are therefore more naturaly accomodated by this equation.
The unknown $\epsilon_k$ term can be handled by removing it entirely, although more sophisticated approaches to solving will utilize a known variance, $\sigma^2$, related to the measurement environment, into the analysis and propogate this error into the final $\phi$.


% ------------------------------------------------------------------------------
\subsection{Proposed Methods of Solution}

% here, i mention the ammount of solution methods and then explain why there is a problem with a math-only approach
% there's a lot of methods
The process of obtaining the solution for the unknown neutron spectrum from the set of possible solutions is known as spectral deconvolution (or unfolding), and given the ammount of valid solutions to the ill-posed problem, the volume of existing deconvolution methods is, perhaps unsurprisingly, commensurate to the number of solutions themselves.
% it is possible to just use a math-based approach
One can feasibly produce a valid solution using a method derived entirely from the realm of mathematics.
% lots of math based methods exist that give acceptable solutions
In fact, an expansive literature exists on pure-math methods to solving ill-posed, linear, inverse problems such as the one posed.
% how can we say if they're any good though?
Though these may produce solutions that satisfy the equation, what can be said of the verisimilitude of these solutions?

% these can produce spectra with features that are unreasonable
Unfortunately, solution spectra derived from these methods can exhibit features that lack a physical basis or be deficient of features present in the actual neutron spectrum.
% this claim can be made because much is already known about the physics in a reactor core
Though the neutron spectrum under inspection is technically unknown, a strong theoretical foundation allows one to make this claim.
% reactors have been studied for a long time
Nearly a century's worth of effort has gone into understanding neutronics and reactor physics, perhaps beginning with James Chadwick's 1932 discovery of the neutron (CITE).
% this has led to the development of a strong theory of reactor theory and neutronics
Since then, this effort has ultimately led to the development of a robust and nearly comprehensive theory of nuclear reactor physics that have allowed us to construct a diverse fleet of hundreds of nuclear fission reactors.
% this knowledge is crutial for discriminating bad or un-physical spectra, without it, there'd be know way to reasonablly accept a solution
It is this knowledge that is crucial for assisting in the discrimination of derived neutron spectra, for without it, there would be no scientifically-grounded criteria for acceptance or rejection.
% this means that math only approaches are no good
Deconvolution methods built solely from mathematic principles must be abandoned.
% to have a good solution method, we need to incorporate this information into our analysis
For a method to bear utility, it must weave this theoretical information into its analysis.

% it is common practice to analize the spectrum after unfolding to see if it appears physically reasonable
A first step in applying this information is through qualitative inspection of the solution spectrum.
% the features in the spectrum should match that of our understanding
It is common practice to study an unfolded flux spectrum and determine whether or not it matches our {\it a priori} understanding of a neutron flux.
% however, this leaves the problem of infinity solutions, and doesn't assist the liklihood that we'll arrive on a reasonable solution
Although this inspection, however useful, can sanction the acceptance of a solution, it does not increase the liklihood that a physically reasonable solution will actually be arrived at.

% constraints can be added and the problem viewed as an optimizaiton problem
Another path to include this information is through constraints imposition.
In this way, the problem is considered an optimization, specifically a minimization of the difference in the measured and calculated responses, subject to certain physical constraints.
% examples of these constraints include avoiding jumps, and other stuff, and these will be discussed later
Examples of these constraints include jump limitation and regional enforcing of increasement or decreasement, and will be discussed in more detail later.

% by far, the most common approach to this dilema is the inclusion of what is called a default spectrum
Undoubtedly, the most ubiquitous solution to the dilema of theory inclusion is the utilization of a `default spectrum'.
% this spectrum, also known as a guess or trial spectrum, is the best initial guess for what the final spectrum could be
This default spectrum, also commonly referred to as a guess spectrum or trial spectrum, is a theoretically-based, {\it a priori} solution hypothesis - the best guess at the final answer.
% it makes sense, since ya know, perfect guess, then we've already got the answer
If we have access to the true solution, cast as a perfect guess, the reliance on the uninformed mathematics behind deconvolution is completely alleviated.
% if the trial spectrum is slightly off, our solution method just has to tweak it slightly to arrive at a solution
If the default spectrum is correct in essense (general shape and magnitude), but exhibits a handful of minor inaccuracies, the task of the unfolding method is {\it tuning}, not building, the true solution.
% this changes the view of unfolding from a ground up production of a solution, to a tweaking process
This refashions the entire problem of unfolding to one of inferrence, where substantial weight is placed on the construction of the greatest default spectrum, and not necessarily on the math-mechanics of the method.

% but can we do this?
Convienient as this appears, the skeptic may inquire as to the viability of such a refashioning.
% it's the a priori understanding we've got of reactor cores that allow us to do this
The response: ``How confident are you in our models?"
% taking for example a typical lwr, a spectrum should generally consist of a superposition of 3 features
Let's take for example a typical light water reactor (LWR).
In the general case, the neutron spectrum present in such a reactor can be justifiably simplified to the superposition of three features.
% these features are the fission neutron spectrum, the 1/e region and the maxwellian distribution
The fast region is comprised of fission neutrons, commonly modeled via Watt's distribution; the thermal end resembles a gas dependent on reactor temperature and can be modeled as a Maxwell-Boltzmann distribution; and downscattering, a slowing down process proportional to the inverse of the neutron energy, affixes the two peaks.
% these all can conveniently be represented mathematically, however, the exact values of certain constants are unknown
The constants associated with any of these models are all physically meaningful and can be selected heuristically, aided by other known reactor parameters, measurements, or standards, which guarantees the absence of guess-work.
% using this as a default spectrum input to an unfolding method, the spectrum can be tweaked to match the detector measurement and the solution is almost guaranteed to be physically reasonable, as it won't be a huge departure from something that is *by defintion* reasonable
Unfolding, viewed as a tuning process and initialized with a modeled guess spectrum such as this, will, with high likelihood, produce a physically valid solution spectrum because it is just a slight alteration to some spectrum that is, {\it by definition}, physically valid.

% do we want more accuracy?
This LWR default spectrum model provides a formidable starting point for unfolding; however, it is deficient in its capacity to capture finer resolution, spatially or temporally dependent, in-core intricacies.
% we also might not be in a lwr reactor core?
The interest in neutron spectrometry is certainly not limited to only in-core applications, so other, more general approaches for hypothesizing a defaults spectrum are necessary.
% even if we're not in one of these locations, we don't have to worry, numerical simulations often can provide us with a guess for the neutron spectrum; these go beyond the scope of just regular analytical solutions
To achieve this level of detail in a default spectrum model, we must turn our attention to numerical approaches, a methodology that allows for a departure from, or maybe, remarkable enhancement to, the usefulness of analytical models.
% modern computers with lots of computing power allow for the modeling of incredibly complex physical features
The computing power available to modern reactor theorists have allowed for the generation of incredibly complex representations of neutronic environments with high fidelity and often reasonable computational times.
% both MC and deterministic methods can be applied to a neturonic problem and obtain a neutron spectrum
Subdivided into two major classes, stochastic and deterministic methods, with both bearing unique advantages, these numerical approaches are the contemporary standard for obtaining a default spectrum.
% this allows us to depart from typical problems into the domain of any understood or semi-understood problem and allows for the application of spectral unfolding methods to almost any concievable physically possible experimental configuration
Programs such as MCNP and Geant have paved a road from the domain of typical or simplified problems into that of any well-understood or semi-understood spectral obtention problem and grant deconvolutions methods value in almost any concievable, physically-possible experimental configuration.

% now to discuss the algorithms themselves
% these algorithms have all been written into codes over and over again, and although that will be discussed, the underlyting algorithm is the important part, so that's what they'll be discussed in terms of
% three primary algorithms will be introduced, SandII, MAXED, and doroshenko



%                   -------------------------------------

\subsubsection{Sand II}

% the sand ii algorithm was introduced in 1967 by McElroy et. al. at air force weapons lab in a paper titled 'A COMPUTER-AUTOMATED V0 ITERATIVE METHOD FOR NEUTRON FLUX SPECTRA DETERMINATIONBY FOIL ACTIVATION'
The Sand II algorithm was first introduced in a 1967 paper from the Air Force Weapons Lab by McElroy et. al. describing `A Computer-Automated Iterative Method for Neutron Flux Spectra determination by Foil Activation'. (CITE)
% the codes purpose was unfolding, specifically in the case of
Sand II was built specifically with foil activation unfolding in mind, using a specially-designed, iterative, spectral adjustment procedure.
% it used a default spectrum as the first step in an iterative scheme
The procedure utilized a user-provided default spectrum, then would use built in cross section libraries to compose the response matrix.

% the algorithm
The algorithm itself is a relatively simple procedure meant to minimize the differences in the measured and calculated responses.
% first it folded the default spectrum and the response functions
First, the default spectrum and response functions are folded together to obtain a set of calculated responses.
% these were compared with each measured response to obtain correction factors for each foil
Then, these are compared with each measured response to obtain correction factors for each foil.
% a weighting function is obtained (for each foil) based on the 'sensitivity function' (differenial cross section times differential flux)
A weighting function, which is just the differential flux times the differential cross sections, is then obtained for each foil.
% an energy dependent correction function is obtained by comparing the weighting functions and the correction factors
An energy dependent correction function is obtained by comparing these weighting functions with each foil's correction factor.
% these correction functions are applied to the flux and the flux is updated
The correction functions are applied to the current step's flux, and an new flux is obtained.

% put the actual equation here and talk about it
The many steps from above can be written in a concise format,

% the sand ii iteration scheme
\begin{equation}\label{eqn:sandii}
\phi_j^{k + 1} = \phi_j^{k} exp(\frac{\sum_i W_{ji}^k \log(\frac{N_i}{\sum_{j'} R_{ij'} \phi_{j'}^k})}{\sum_i W_{ji}^k}) ,
\end{equation}

where the weighting function is provided by

% the sand ii weighting function
\begin{equation}\label{eqn:sandii-w}
W_{ji}^k = \frac{R_{ij} \phi_{j}^k}{\sum_{j'} R_{ij'} \phi_{j'}^k} .
\end{equation}

% discuss the solution acceptability
Because Sand II is iterative, there needs to be a criteria defined for solution acceptability.
% then how close successive iterations will converge on an acceptable solution
A user provides a value which represents the degree to which each differential flux value for two successive iterations must agree for a solution to be considered achieved.

% initially, since its purpose was foil activation, it imposed a bin structure with 621 points between 10**-10 and 18 MeV.
Because the purpose of the code was foil activation, a bin structure is imposed with 621 points between $10^{-10}$ to $18$ MeV.
% 29 foil activations came with it, too since its purpose was foil activation, those were built in
A library is distibuted with the code that contained, originally, 29 foil activation cross sections, plus built-in covers that could be applied to correct the cross sections.

% also mention how sand ii has a rejection criterion
There is also a neat feature built into the code that will, given a user defined value, discard an individual foil measurement that is a given number of standard deviations away from the calculated value using the solution spectrum.
This will cause the solution to be recomputed without this measured value.

% other codes
The Sand II algorithm has seen a lot of application since its introduction in the sixties.
Many other codes have built upon the foundation of the algorithm since then, such as Sand IV and MSANDB.
The popular unfolding code, Gravel, utilizes the same algorithm, but includes one small change.
The weighting function is updated like so,

% the modified gravel weighting function
\begin{equation}\label{eqn:gravel-w}
W_{ji}^k = \frac{R_{ij} \phi_{j}^k}{\sum_{j'} R_{ij'} \phi_{j'}^k} \frac{N_i^2}{\sigma_i^2} ,
\end{equation}

to include a term that incorporates the measurement errors into the solution finding process.



%                   -------------------------------------
\subsubsection{Doroshenko et. al.}

% introduction
% doroshenko et. al. released an algorithm
In 1975, an algorithm was introduced by Doroshenko et. al. to tackle the problem of spectral unfolding.
% the basis of the algorithm was unfolding via maximization of `directed divergence'
The basis of the algorithm was to provide a solution that satisfies \EQN{eqn:disc-response} while also maximizing a quantity called `directed divergence'.
% directed divergence, or entropy is an information measure that is related to the likelihood of a neutron spectrum taking a particular shape
Directed divergence, also referred to as entropy, is a measure related to the liklihood that a neutron spectrum will take a particular shape.
A more complete treatement of entropy will be handled later.
% in the case of doroshenko's algorithm, the following method of entropy calculation is used
In the case of Doroshenko's algorithm, the following method of entropy calculation is used

% entropy equation used in doroshenko
\begin{equation}\label{eqn:doroshenko-entropy}
J = - \sum_i N_i \ln(\frac{N_i}{\sum_j \phi_j R_{ij}}) .
\end{equation}

% from this definition, an iterative scheme can be derived that will tend to converge on a flux spectrum that maximized the entropy
An iterative scheme can then be derived that will, after sufficient iteration, converge on a flux spectrum that will maximize the entropy $J$ of the spectrum.
% this is shown in the following equation (discuss it)
This relationship is shown in the following equation,

% the doroshenko iteration scheme
\begin{equation}\label{eqn:doroshenko}
\phi_j^{k + 1} = \phi_j^{k} (\frac{\sum_i \frac{R_{ji}}{\sum_j R_{ij} \phi_j^k}}{\sum_i \frac{R_{ji}}{N_i}}) .
\end{equation}

% discuss the algorithm
Over iterations, this algorithm will compute the ratio between calculated and measured responses, similar to the correction factors described in the Sand II algorithm.
These are used to produce a weighting function that will correct the flux more in the regions that produce the largest deviations in calculated-to-measured responses.
A measure of error can be computed to compare responses,

% the doroshenko iteration scheme
\begin{equation}\label{eqn:doroshenko-error}
\chi^2 = \frac{1}{N} \sum_{i=1}^N [\frac{\sum_j R_{ij} \phi_j^k - N_i}{N_i}]^2 ,
\end{equation}

which will tend to rapidly converge on a finite value, so the change in $\chi$ is used as a convergence criteria,

% the doroshenko iteration scheme
\begin{equation}\label{eqn:doroshenko-error-delta}
\delta \chi^k = \chi^k - \chi^{k-1} .
\end{equation}

% discuss positivity, etc.
Similar to Sand II, the solution found in this way is guaranteed to be positive, given $R$ is positive.
One advantage exhibited by this algorithm is it's relative simplicity and easiness to implement.
It's basis in infomation theory means it has a grounding in mathematics that is related to neutronic behavior, and obvious advantage for the application.

% talk about different codes that use doroshenko
The unfolding code SPUNIT was one of the earlier codes to apply the Doroshenko method, and since then it has been used in several other unfolding codes, such as the popular BUNKI, as well as AFITBUNKI and NSDUAZ.



%                   -------------------------------------

\subsubsection{MAXED}

% introduction
Other maximum entropy unfolding methods have been proposed since Doroshenko, such as Itoh and Tsunoda (1989), but in 1998, a fortran based code from Reginatto and Goldhagen was introduced that has been widely used and become very popular since then.

This algorithm begins with \EQN{eqn:disc-response} but adds an additional constraint

\begin{equation}\label{eqn:maxed-omega}
\Omega = \sum_i \frac{\epsilon_i^2}{\sigma_i^2} ,
\end{equation}

where $\Omega$ is a user-selected parameter that relates the measurement error of the detectors to the allowable deviation in the measured and calculated responses.

The unique feature of MAXED is that entropy, while normally considered a measure of the solution spectrum itself, is here instead characterized as a value associated with the deviation between the default and solution spectra.
The code will then search for the maximum entropy solution that is most similar to the default spectrum using the following equation for entropy,

\begin{equation}\label{eqn:maxed-skilling}
S = - \sum [\phi_j \ln (\frac{\phi_j}{\phi_j^{DEF}}) + \phi_j^{DEF} - \phi_j] .
\end{equation}

In this way, the in-built hypothesis in the starting spectrum are more likely conserved, as they are considered parts of the $a priori$ information inclusion process described earlier.

To solve this equation, it first can be shown the the maximization of the entropy as defined above is associatied with the Lagrangian,

\begin{equation}\label{eqn:maxed-lagrangian}
L(\phi_j, \epsilon_i, \lambda_i, \mu) = -\sum [\phi_j \ln (\frac{\phi_j}{\phi_j^{DEF}}) + \phi_j^{DEF} - \phi_j] - \sum \lambda_i [\sum R_{ij} \phi_j - N_i - \epsilon_i] - \mu [\sum (\frac{\epsilon_i}{\sigma_k})^2 - \Omega] ,
\end{equation}

and that $\lamda_i$ and $\mu$ are Lagrange multipliers.
Reginatto and Goldhagen show that variation of $L$ is equivalent to the maximization of the following potential function, $Z$,

\begin{equation}\label{eqn:maxed-potential}
Z = \sum \phi_j^{DEF} \exp(-\sum \lambda_i R_{ij}) - [\Omega \sum (\lambda_i \sigma_i)^2]^{1/2} - \sum N_i \lambda_i .
\end{equation}

Although many methods can be applied to maximize the function, the chosen method for MAXED was simulated annealing for its ability to discriminate between local and global optima.
This process will give the $\lambda_i$ that maximize $Z$ which then can be used in the following two relationships to provide a solution flux and errors associated each response,

\begin{equation}\label{eqn:maxed-flux}
\phi_j = \phi_j^{DEF} \exp(- \sum_i \lambda_k R_{ij}) ,
\end{equation}

\begin{equation}\label{eqn:maxed-error}
\epsilon_i = \frac{\lambda_i \sigma_i^2}{2} (\frac{4 \Omega}{\sum_j (\lambda_j \sigma_j)^2})^{1/2} .
\end{equation}

% conculsions
The method is shown to work in a variety of cases and been included in many comparitive studies.
Much of the emphasis in Reginatto and Goldhagen's work is on the importance of inference in obtaining the solution.
Without proper $a priori$ information, the solution should not be considered useful.


%                   -------------------------------------

\subsubsection{Other Methods}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Variance Reduction Methods}

